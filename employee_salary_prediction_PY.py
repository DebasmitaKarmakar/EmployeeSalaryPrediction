# -*- coding: utf-8 -*-
"""EMPLOYEE SALARY PREDICTION (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwtIXo4bseUf5Qf-xC6VDd9zKmeDeOtj
"""

#Employee salary prediction using adult csv
#load ur libraries

!pip install pandas

!pip install seaborn

import pandas as pd

"""from google.colab import drive
drive.mount('/content/drive')



"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)
print("‚úÖ Dataset loaded successfully!")

data

data.shape
#rows and columns shows

data.head()
#upr se dikhayega

data.head(7)

data.tail()
# niche se dikhayega

data.tail(7)

#findinf null values
#isna= is there any null value
data.isna()
#if true then null value is there, if false, then no null value

data.isna().sum()
#retrive all the null values under each column
#if u dont have any null values u r good to go wit the dataset

#lets take the first variable i.e workclass
#we will undrstnd, the value under each column is category values or any other value
#we will use the method called value count
#categorical data
print(data.occupation.value_counts())
#the result shows which one is dominating in the occupation section
#there is a questionmark, what we need to do is remove the special character with value

print(data.gender.value_counts())

print(data.education.value_counts())

print(data.workclass.value_counts())
#without pay or never worked wont actually contribute to salary prediction

#remove these two data..we consider it as dimenionality reduction
#to access the workclass, we we consider data of workclass

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data.workclass.value_counts())

print(data['marital-status'].value_counts())

print(data['age'].value_counts())
#80+ is not contributing, so we can remove them to speed up the process

#there is a questionmark, what we need to do is remove the special character with value
#we will replace it will others category
#Replacing the name of the category with some other suitable name

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Replace with your Drive path
file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)

# Replace '?' with 'NA'
data['occupation'] = data['occupation'].replace({'?': 'NA'})

# Encode occupation names
encoder = LabelEncoder()
data['occupation_encoded'] = encoder.fit_transform(data['occupation'])

# Decode to check
data['occupation_decoded'] = encoder.inverse_transform(data['occupation_encoded'])

# Display results
print(data[['occupation', 'occupation_encoded', 'occupation_decoded']].head())

print(data['occupation'].value_counts())

data.shape

#REDUNDENCY
#TWO DIFF COLUMNS CONVEYING THE SAME INFORMATION
#here education and educational no. is doing the same
#we will remove education cz it is categorical value..nd we prefer numerical value
#drop is the function used to delete a particular column from the dataset

data.drop(columns=['education'],inplace=True)

data.drop(columns=['fnlwgt'],inplace=True)

data

#OUTLIER
import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()
#this shows numerical values not the catagorical values
#from 80 to 90, the circle rep outliers
#20 to 30 freshers
#25 to 45 adult experienced employees
#45 to 60 senior employees
#most important employees : 25 to 45 the box represents

#considerable range
data=data[(data['age']<=75)&(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()
#removing the outliers

# Commented out IPython magic to ensure Python compatibility.
# %pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy scipy scikit-learn --force-reinstall

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load dataset
# Replace with your Drive path
file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)  # Replace with your file path

# Create LabelEncoder instance
#assign some unique no. to each category of the column
encoder = LabelEncoder()

# Encode categorical columns
data['workclass'] = encoder.fit_transform(data['workclass'])
data['marital-status'] = encoder.fit_transform(data['marital-status'])
data['occupation'] = encoder.fit_transform(data['occupation'])
data['relationship'] = encoder.fit_transform(data['relationship'])
data['race'] = encoder.fit_transform(data['race'])
data['gender'] = encoder.fit_transform(data['gender'])
data['native-country'] = encoder.fit_transform(data['native-country'])

# Display the encoded dataset
print(data.head())

from sklearn.preprocessing import LabelEncoder

# Create label encoders
le_education = LabelEncoder()
le_job = LabelEncoder()

# Fit and transform categorical columns
data['education'] = le_education.fit_transform(data['education'])

# Save encoders for later use in the app
import pickle
with open('le_education.pkl', 'wb') as f:
    pickle.dump(le_education, f)

#first we need to do splitting of data into training and testing

#Dividing the data set into two parts : Input and Output
 x = data.drop(columns=['income']) #input
 y = data['income'] #output

x

y

print(x.dtypes)

#education in in object type, but we need it in int64 to get the desired outcome
from sklearn.preprocessing import LabelEncoder

# Make a copy to avoid modifying the original
x_encoded = x.copy()

# Encode 'education' column
le = LabelEncoder()
x_encoded['education'] = le.fit_transform(x_encoded['education'])

# Now 'education' will be int64
print(x_encoded.dtypes)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x = scaler.fit_transform(x_encoded)
x

# Back to DataFrame if needed
x_df = pd.DataFrame(x, columns=x_encoded.columns)

#for training and testing
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=43, stratify=y)
# x ones are input training data, we can give any no. for the random state, stratify will evenly distribute the values
# test_size is test data split up ratio, which is default ratio is 20 % thats why given 0.2
# random state ensures for any given condition the ratio should be 20-80
# xtrain and ytrain is used to make the machine learn patterns
# xtest and ytest is used to see if my model is working properly or not

xtrain
# successfully split up the data for training and testing

# creating a ml algorithm - supervised machine learning algorithm
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain, ytrain) #input and output training data
predict=knn.predict(xtest)
predict

# after prediction it is the turn of result and evaluation
# ytest is the key to predict the answers

# 3 times with different way we will predict the accuracy

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)

#logistic regretion
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(xtrain, ytrain) #input and output training data
predict1=lr.predict(xtest)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict1)

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), random_state=2, max_iter=2000)
clf.fit(xtrain, ytrain)
predict2=lr.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {
    "LogisticRegression" : LogisticRegression(),
    "RandomForest" : RandomForestClassifier(),
    "KNN" : KNeighborsClassifier(),
    "SVM" : SVC(),
    "GradientBoosting" : GradientBoostingClassifier()
}

results = {}

for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

import pandas as pd

file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)

# Show first few rows
print("‚úÖ Data loaded successfully:")
data.head()

data.replace("?", pd.NA, inplace=True)
data.dropna(inplace=True)  # or use fillna()

data_encoded = pd.get_dummies(data, drop_first=True)

X = data_encoded.drop("income_>50K", axis=1)  # target is income
y = data_encoded["income_>50K"]  # 1 if >50K, else 0

!pip install seaborn

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.boxplot(x='income', y='age', data=data)
plt.title("Salary vs Age")
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(x='education', y='hours-per-week', hue='income', data=data)
plt.xticks(rotation=45)
plt.title("Salary vs Education")
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(x='gender', y='hours-per-week', hue='income', data=data)
plt.title("Gender Pay Gap Analysis")
plt.show()

top_countries = data['native-country'].value_counts().head(10).index
data_top_countries = data[data['native-country'].isin(top_countries)]

plt.figure(figsize=(12,6))
sns.countplot(y='native-country', hue='income', data=data_top_countries)
plt.title("Salary Distribution by Country")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

import numpy as np

# Get importances and features
importances = model.feature_importances_
features = X.columns

# Sort features and importances
sorted_idx = np.argsort(importances)[::-1]
sorted_features = features[sorted_idx]
sorted_importances = importances[sorted_idx]

import numpy as np
import matplotlib.pyplot as plt

# Sort features by importance
importances = model.feature_importances_
features = X.columns
sorted_idx = np.argsort(importances)[::-1]

# Plot Top 15 Features
top_n = 15
plt.figure(figsize=(10,6))
plt.barh(features[sorted_idx][:top_n][::-1], importances[sorted_idx][:top_n][::-1])
plt.title("Top 14 Feature Importances from RandomForest")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

import joblib
from sklearn.preprocessing import LabelEncoder

columns_to_encode = ['education', 'occupation', 'workclass', 'gender']
encoders = {}

for col in columns_to_encode:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    encoders[col] = le
    joblib.dump(le, f'le_{col}.pkl')  # Save encoder for each column

# Now train your model and save it as `best_model.pkl`

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import joblib
import pandas as pd

# Load dataset
file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)

# Select the 7 features you want
data['experience'] = data['age'] - 18
features = ['age', 'education', 'occupation', 'workclass', 'gender', 'hours-per-week', 'experience']
target = 'income'  # Replace with the actual target column name in your dataset

# Label Encode categorical features
encoders = {}
for col in ['education', 'occupation', 'workclass', 'gender']:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    encoders[col] = le
    joblib.dump(le, f'le_{col}.pkl')

# Train-test split
X = data[features]
y = data[target]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to balance training data
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Define models
models = {
    "LogisticRegression": LogisticRegression(class_weight='balanced', max_iter=1000),
    "RandomForest": RandomForestClassifier(class_weight='balanced', random_state=42),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train_res, y_train_res)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, preds))
    print("-" * 50)

# Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\n‚úÖ Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model, "best_model.pkl")
print("‚úÖ Saved best model as best_model.pkl")
print("‚úÖ Model retrained and saved with 7 features.")

# from sklearn.metrics import classification_report

import matplotlib.pyplot as plt

# Plot model accuracy comparison
plt.figure(figsize=(8,6))
plt.bar(results.keys(), results.values(), color='red')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xlabel("Models")
plt.ylim(0,1)
plt.grid(axis='y')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# import joblib
# import requests
# import os
# 
# # Google Drive direct link
# url = "https://drive.google.com/uc?id=1dhtcPk1KT6fsWy0jilKOB3B-YwuNuMqZ"
# filename = "best_model.pkl"
# 
# # Download only if not already present
# if not os.path.exists(filename):
#     print("üì• Downloading model from Google Drive...")
#     with requests.get(url, stream=True) as r:
#         with open(filename, 'wb') as f:
#             f.write(r.content)
#     print("‚úÖ Model downloaded successfully.")
# 
# # Load the model
# model = joblib.load(filename)
# 
# 
# # Load model and encoders
# model = joblib.load("best_model.pkl")
# le_education = joblib.load("le_education.pkl")
# le_occupation = joblib.load("le_occupation.pkl")
# le_workclass = joblib.load("le_workclass.pkl")
# le_gender = joblib.load("le_gender.pkl")
# # Load any other encoders if needed
# 
# # Streamlit config
# st.set_page_config(page_title="WageWise: Smart Salary Analyzer", page_icon="üíº", layout="centered")
# st.title("üíº WageWise: Smart Salary Analyzer")
# st.markdown("Predict how much an employee earns based on their details. Gain quick insights into what factors drive salaries in the organization.")
# 
# # Sidebar
# st.sidebar.header("Enter Employee Details")
# st.image("https://upload.wikimedia.org/wikipedia/commons/5/51/IBM_logo.svg", width=120)
# 
# # User inputs
# # Sidebar inputs
# st.sidebar.header("Enter Employee Details")
# age = st.sidebar.slider("Age", 18, 65, 30)
# education = st.sidebar.selectbox("Education Level", le_education.classes_)
# occupation = st.sidebar.selectbox("Occupation", le_occupation.classes_)
# workclass = st.sidebar.selectbox("Workclass", le_workclass.classes_)
# gender = st.sidebar.selectbox("Gender", le_gender.classes_)
# hours_per_week = st.sidebar.slider("Hours per week", 1, 80, 40)
# experience = st.sidebar.slider("Years of Experience", 0, 40, 5)
# 
# # Build input Dataframe (must match preprocessing of your training data)
# input_df = pd.DataFrame({
#     'age': [age],
#     'education': [education],
#     'occupation': [occupation],
#     'workclass': [workclass],
#     'gender': [gender],
#     'hours-per-week': [hours_per_week],
#     'experience': [experience]
# })
# 
# st.write("### Input Summary")
# st.table(input_df)
# 
# # Preprocess function
# def preprocess_input(df):
#     df = df.copy()
#     df['education'] = le_education.transform(df['education'])
#     df['occupation'] = le_occupation.transform(df['occupation'])
#     df['workclass'] = le_workclass.transform(df['workclass'])
#     df['gender'] = le_gender.transform(df['gender'])
#     return df
# 
# # Prediction section
# if st.button("üîÆ Predict Salary Class"):
#     try:
#         processed = preprocess_input(input_df)
#         prediction = model.predict(processed)
#         probability = model.predict_proba(processed).max() * 100 if hasattr(model, 'predict_proba') else None
# 
#         if prediction[0] == '>50K':
#             st.success("This employee is likely to earn **above $50K/year**.")
#         else:
#             st.warning("This employee is likely to earn **$50K/year or less**.")
# 
#         if probability:
#             st.markdown(f"**Confidence Level:** {probability:.2f}%")
# 
#         st.balloons()
#     except Exception as e:
#         st.error(f"Error during prediction: {str(e)}")
# 
# # Batch Prediction Section
# st.markdown("---")
# st.markdown("### üìÇ Batch Prediction for Multiple Employees")
# 
# uploaded_file = st.file_uploader("Upload a CSV file for salary prediction", type="csv")
# 
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("üìÑ Uploaded Data Preview:", batch_data.head())
#     st.dataframe(batch_data.head())
# 
#     try:
#         processed_data = preprocess_input(batch_data)
#         batch_preds = model.predict(processed_data)
#         batch_data['PredictedClass'] = batch_preds
# 
#         above_50k = (batch_preds == '>50K').sum()
#         total = len(batch_preds)
#         st.markdown(f"‚úÖ **{above_50k}/{total} employees** predicted to earn >50K.")
# 
#         st.write("üìä Predictions with added column:")
#         st.dataframe(batch_data.head())
# 
#         csv = batch_data.to_csv(index=False).encode('utf-8')
#         st.download_button("üì• Download Predictions CSV", csv, file_name='salary_predictions.csv', mime='text/csv')
# 
#     except Exception as e:
#         st.error(f"‚ùå Error during prediction: {str(e)}")
# 
# st.markdown("---")
# st.markdown("üë©‚Äçüíª *Built with ‚ù§Ô∏è for HR professionals and data enthusiasts.*")
#

!pip install streamlit pyngrok

!ngrok authtoken 309KFcPtopLpMIdnfjFPw7XLBsW_67HT4VY7gGeeYZ6Yhd5nu

import os
import threading

def run_streamlit():
    os.system('streamlit run app.py --server.port 8501')

thread = threading.Thread(target = run_streamlit)
thread.start()

from pyngrok import ngrok
import time

#wait a few sec
time.sleep(5)

#create a tunnel to streamlit port 8501
public_url = ngrok.connect(8501)
print("Your Streamlit app is live here:", public_url)

!streamlit run app.py

#ERROR CORRECTION BEGINS- PREVIOUSLY BIASED MODEL

data['income'].value_counts()

print(model.predict(X_test[:10]))
print(y_test[:10])

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='income', data=data)
plt.title("Income Class Distribution")
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
data['income'] = le.fit_transform(data['income'])  # This should be done after checking distribution

from sklearn.preprocessing import LabelEncoder
import joblib
import os

# Make sure the directory exists
os.makedirs("model", exist_ok=True)

# Assuming your DataFrame is named `data` and is loaded

columns_to_encode = ['education', 'occupation', 'workclass', 'gender']
for col in columns_to_encode:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    joblib.dump(le, f"model/le_{col}.pkl")  # Save each encoder inside the model/ folder
    joblib.dump(best_model, "model/best_model.pkl")

model = joblib.load("model/best_model.pkl")
le_education = joblib.load("model/le_education.pkl")
le_occupation = joblib.load("model/le_occupation.pkl")
le_workclass = joblib.load("model/le_workclass.pkl")
le_gender = joblib.load("model/le_gender.pkl")

from google.colab import files
files.download("model/best_model.pkl")
files.download("model/le_education.pkl")
files.download("model/le_occupation.pkl")
files.download("model/le_workclass.pkl")
files.download("model/le_gender.pkl")

files.download("app.py")